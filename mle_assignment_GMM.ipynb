{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ddaf923-f7eb-408c-ae66-4c446411e034",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "import time\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from distributed import Client\n",
    "import dask.dataframe as dd\n",
    "import psutil\n",
    "import gc\n",
    "import logging\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "195c4b4e-cf62-4ce9-9ed9-d1e34c85ceba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformer():\n",
    "    \n",
    "    \"\"\"\n",
    "    Transformer class responsible for processing data to train the CTABGANSynthesizer model\n",
    "    \n",
    "    Variables:\n",
    "    1) train_data -> input dataframe \n",
    "    2) categorical_list -> list of categorical columns\n",
    "    3) mixed_dict -> dictionary of mixed columns\n",
    "    4) n_clusters -> number of modes to fit bayesian gaussian mixture (bgm) model\n",
    "    5) eps -> threshold for ignoring less prominent modes in the mixture model \n",
    "    6) ordering -> stores original ordering for modes of numeric columns\n",
    "    7) output_info -> stores dimension and output activations of columns (i.e., tanh for numeric, softmax for categorical)\n",
    "    8) output_dim -> stores the final column width of the transformed data\n",
    "    9) components -> stores the valid modes used by numeric columns\n",
    "    10) filter_arr -> stores valid indices of continuous component in mixed columns\n",
    "    11) meta -> stores column information corresponding to different data types i.e., categorical/mixed/numerical\n",
    "\n",
    "\n",
    "    Methods:\n",
    "    1) __init__() -> initializes transformer object and computes meta information of columns\n",
    "    2) get_metadata() -> builds an inventory of individual columns and stores their relevant properties\n",
    "    3) fit() -> fits the required bgm models to process the input data\n",
    "    4) transform() -> executes the transformation required to train the model\n",
    "    5) inverse_transform() -> executes the reverse transformation on data generated from the model\n",
    "    \n",
    "    \"\"\"    \n",
    "    \n",
    "    def __init__(self, train_data=pd.DataFrame, categorical_list=[], mixed_dict={}, n_clusters=10, eps=0.005):\n",
    "        \n",
    "        self.meta = None\n",
    "        self.train_data = train_data\n",
    "        self.categorical_columns= categorical_list\n",
    "        self.mixed_columns= mixed_dict\n",
    "        self.n_clusters = n_clusters\n",
    "        self.eps = eps\n",
    "        self.ordering = []\n",
    "        self.output_info = []\n",
    "        self.output_dim = 0\n",
    "        self.components = []\n",
    "        self.filter_arr = []\n",
    "        self.meta = self.get_metadata()\n",
    "        \n",
    "    def get_metadata(self):\n",
    "        \n",
    "        meta = []\n",
    "    \n",
    "        for index in range(self.train_data.shape[1]):\n",
    "            column = self.train_data.iloc[:,index]\n",
    "            if index in self.categorical_columns:\n",
    "                mapper = column.value_counts().index.tolist()\n",
    "                meta.append({\n",
    "                        \"name\": index,\n",
    "                        \"type\": \"categorical\",\n",
    "                        \"size\": len(mapper),\n",
    "                        \"i2s\": mapper\n",
    "                })\n",
    "            elif index in self.mixed_columns.keys():\n",
    "                meta.append({\n",
    "                    \"name\": index,\n",
    "                    \"type\": \"mixed\",\n",
    "                    \"min\": column.min(),\n",
    "                    \"max\": column.max(),\n",
    "                    \"modal\": self.mixed_columns[index]\n",
    "                })\n",
    "            else:\n",
    "                meta.append({\n",
    "                    \"name\": index,\n",
    "                    \"type\": \"continuous\",\n",
    "                    \"min\": column.min(),\n",
    "                    \"max\": column.max(),\n",
    "                })            \n",
    "\n",
    "        return meta\n",
    "\n",
    "    def fit(self):\n",
    "        \n",
    "        data = self.train_data.values\n",
    "        \n",
    "        # stores the corresponding bgm models for processing numeric data\n",
    "        model = []\n",
    "        \n",
    "        # iterating through column information\n",
    "        for id_, info in enumerate(self.meta):\n",
    "            if info['type'] == \"continuous\":\n",
    "                # fitting bgm model  \n",
    "                gm = BayesianGaussianMixture(\n",
    "                    n_components = self.n_clusters, \n",
    "                    weight_concentration_prior_type='dirichlet_process',\n",
    "                    weight_concentration_prior=0.001, # lower values result in lesser modes being active\n",
    "                    max_iter=100,n_init=1, random_state=42)\n",
    "                gm.fit(data[:, id_].reshape([-1, 1]))\n",
    "                model.append(gm)\n",
    "                # keeping only relevant modes that have higher weight than eps and are used to fit the data\n",
    "                old_comp = gm.weights_ > self.eps\n",
    "                mode_freq = (pd.Series(gm.predict(data[:, id_].reshape([-1, 1]))).value_counts().keys())\n",
    "                comp = []\n",
    "                for i in range(self.n_clusters):\n",
    "                    if (i in (mode_freq)) & old_comp[i]:\n",
    "                        comp.append(True)\n",
    "                    else:\n",
    "                        comp.append(False)\n",
    "                self.components.append(comp) \n",
    "                self.output_info += [(1, 'tanh'), (np.sum(comp), 'softmax')]\n",
    "                self.output_dim += 1 + np.sum(comp)\n",
    "                \n",
    "            elif info['type'] == \"mixed\":\n",
    "                \n",
    "                # in case of mixed columns, two bgm models are used\n",
    "                gm1 = BayesianGaussianMixture(\n",
    "                    self.n_clusters, \n",
    "                    weight_concentration_prior_type='dirichlet_process',\n",
    "                    weight_concentration_prior=0.001, max_iter=100,\n",
    "                    n_init=1,random_state=42)\n",
    "                gm2 = BayesianGaussianMixture(\n",
    "                    self.n_clusters,\n",
    "                    weight_concentration_prior_type='dirichlet_process',\n",
    "                    weight_concentration_prior=0.001, max_iter=100,\n",
    "                    n_init=1,random_state=42)\n",
    "                \n",
    "                # first bgm model is fit to the entire data only for the purposes of obtaining a normalized value of any particular categorical mode\n",
    "                gm1.fit(data[:, id_].reshape([-1, 1]))\n",
    "                \n",
    "                # main bgm model used to fit the continuous component and serves the same purpose as with purely numeric columns\n",
    "                filter_arr = []\n",
    "                for element in data[:, id_]:\n",
    "                    if element not in info['modal']:\n",
    "                        filter_arr.append(True)\n",
    "                    else:\n",
    "                        filter_arr.append(False)\n",
    "                self.filter_arr.append(filter_arr)\n",
    "                \n",
    "                gm2.fit(data[:, id_][filter_arr].reshape([-1, 1]))\n",
    "                \n",
    "                model.append((gm1,gm2))\n",
    "                \n",
    "                # similarly keeping only relevant modes with higher weight than eps and are used to fit strictly continuous data \n",
    "                old_comp = gm2.weights_ > self.eps\n",
    "                mode_freq = (pd.Series(gm2.predict(data[:, id_][filter_arr].reshape([-1, 1]))).value_counts().keys())  \n",
    "                comp = []\n",
    "                  \n",
    "                for i in range(self.n_clusters):\n",
    "                    if (i in (mode_freq)) & old_comp[i]:\n",
    "                        comp.append(True)\n",
    "                    else:\n",
    "                        comp.append(False)\n",
    "\n",
    "                self.components.append(comp)\n",
    "                \n",
    "                # modes of the categorical component are appended to modes produced by the main bgm model\n",
    "                self.output_info += [(1, 'tanh'), (np.sum(comp) + len(info['modal']), 'softmax')]\n",
    "                self.output_dim += 1 + np.sum(comp) + len(info['modal'])\n",
    "            \n",
    "            else:\n",
    "                # in case of categorical columns, bgm model is ignored\n",
    "                model.append(None)\n",
    "                self.components.append(None)\n",
    "                self.output_info += [(info['size'], 'softmax')]\n",
    "                self.output_dim += info['size']\n",
    "        \n",
    "        self.model = model\n",
    "\n",
    "    def transform(self, data):\n",
    "        \n",
    "        # stores the transformed values\n",
    "        values = []\n",
    "\n",
    "        # used for accessing filter_arr for transforming mixed columns\n",
    "        mixed_counter = 0\n",
    "        \n",
    "        # iterating through column information\n",
    "        for id_, info in enumerate(self.meta):\n",
    "            current = data[:, id_]\n",
    "            if info['type'] == \"continuous\":\n",
    "                # mode-specific normalization occurs here\n",
    "                current = current.reshape([-1, 1])\n",
    "                # means and stds of the modes are obtained from the corresponding fitted bgm model\n",
    "                means = self.model[id_].means_.reshape((1, self.n_clusters))\n",
    "                stds = np.sqrt(self.model[id_].covariances_).reshape((1, self.n_clusters))\n",
    "                # values are then normalized and stored for all modes\n",
    "                features = np.empty(shape=(len(current),self.n_clusters))\n",
    "                # note 4 is a multiplier to ensure values lie between -1 to 1 but this is not always guaranteed\n",
    "                features = (current - means) / (4 * stds) \n",
    "\n",
    "                # number of distict modes\n",
    "                n_opts = sum(self.components[id_])                \n",
    "                # storing the mode for each data point by sampling from the probability mass distribution across all modes based on fitted bgm model \n",
    "                opt_sel = np.zeros(len(data), dtype='int')\n",
    "                probs = self.model[id_].predict_proba(current.reshape([-1, 1]))\n",
    "                probs = probs[:, self.components[id_]]\n",
    "                for i in range(len(data)):\n",
    "                    pp = probs[i] + 1e-6\n",
    "                    pp = pp / sum(pp)\n",
    "                    opt_sel[i] = np.random.choice(np.arange(n_opts), p=pp)\n",
    "                \n",
    "                # creating a one-hot-encoding for the corresponding selected modes\n",
    "                probs_onehot = np.zeros_like(probs)\n",
    "                probs_onehot[np.arange(len(probs)), opt_sel] = 1\n",
    "\n",
    "                # obtaining the normalized values based on the appropriately selected mode and clipping to ensure values are within (-1,1)\n",
    "                idx = np.arange((len(features)))\n",
    "                features = features[:, self.components[id_]]\n",
    "                features = features[idx, opt_sel].reshape([-1, 1])\n",
    "                features = np.clip(features, -.99, .99) \n",
    "                \n",
    "                # re-ordering the one-hot-encoding of modes in descending order as per their frequency of being selected\n",
    "                re_ordered_phot = np.zeros_like(probs_onehot)  \n",
    "                col_sums = probs_onehot.sum(axis=0)\n",
    "                n = probs_onehot.shape[1]\n",
    "                largest_indices = np.argsort(-1*col_sums)[:n]\n",
    "                for id,val in enumerate(largest_indices):\n",
    "                    re_ordered_phot[:,id] = probs_onehot[:,val]\n",
    "                \n",
    "                # storing the original ordering for invoking inverse transform\n",
    "                self.ordering.append(largest_indices)\n",
    "                \n",
    "                # storing transformed numeric column represented as normalized values and corresponding modes \n",
    "                values += [features, re_ordered_phot]\n",
    "                  \n",
    "            elif info['type'] == \"mixed\":\n",
    "                \n",
    "                # means and standard deviation of modes obtained from the first fitted bgm model\n",
    "                means_0 = self.model[id_][0].means_.reshape([-1])\n",
    "                stds_0 = np.sqrt(self.model[id_][0].covariances_).reshape([-1])\n",
    "\n",
    "                # list to store relevant bgm modes for categorical components\n",
    "                zero_std_list = []\n",
    "                \n",
    "                # means and stds needed to normalize relevant categorical components\n",
    "                means_needed = []\n",
    "                stds_needed = []\n",
    "\n",
    "                # obtaining the closest bgm mode to the categorical component\n",
    "                for mode in info['modal']:\n",
    "                    # skipped for mode representing missing values\n",
    "                    if mode!=-9999999:\n",
    "                        dist = []\n",
    "                        for idx,val in enumerate(list(means_0.flatten())):\n",
    "                            dist.append(abs(mode-val))\n",
    "                        index_min = np.argmin(np.array(dist))\n",
    "                        zero_std_list.append(index_min)\n",
    "                    else: continue\n",
    "\n",
    "                \n",
    "                # stores the appropriate normalized value of categorical modes\n",
    "                mode_vals = []\n",
    "                \n",
    "                # based on the means and stds of the chosen modes for categorical components, their respective values are similarly normalized\n",
    "                for idx in zero_std_list:\n",
    "                    means_needed.append(means_0[idx])\n",
    "                    stds_needed.append(stds_0[idx])\n",
    "               \n",
    "                for i,j,k in zip(info['modal'],means_needed,stds_needed):\n",
    "                    this_val  = np.clip(((i - j) / (4*k)), -.99, .99) \n",
    "                    mode_vals.append(this_val)\n",
    "                \n",
    "                # for categorical modes representing missing values, the normalized value associated is simply 0\n",
    "                if -9999999 in info[\"modal\"]:\n",
    "                    mode_vals.append(0)\n",
    "                \n",
    "                # transforming continuous component of mixed columns similar to purely numeric columns using second fitted bgm model\n",
    "                current = current.reshape([-1, 1])\n",
    "                filter_arr = self.filter_arr[mixed_counter]\n",
    "                current = current[filter_arr]\n",
    "    \n",
    "                means = self.model[id_][1].means_.reshape((1, self.n_clusters))\n",
    "                stds = np.sqrt(self.model[id_][1].covariances_).reshape((1, self.n_clusters))\n",
    "                \n",
    "                features = np.empty(shape=(len(current),self.n_clusters))\n",
    "                features = (current - means) / (4 * stds)\n",
    "                \n",
    "                n_opts = sum(self.components[id_]) \n",
    "                probs = self.model[id_][1].predict_proba(current.reshape([-1, 1]))\n",
    "                probs = probs[:, self.components[id_]]\n",
    "                \n",
    "                opt_sel = np.zeros(len(current), dtype='int')\n",
    "                for i in range(len(current)):\n",
    "                    pp = probs[i] + 1e-6\n",
    "                    pp = pp / sum(pp)\n",
    "                    opt_sel[i] = np.random.choice(np.arange(n_opts), p=pp)\n",
    "                \n",
    "                idx = np.arange((len(features)))\n",
    "                features = features[:, self.components[id_]]\n",
    "                features = features[idx, opt_sel].reshape([-1, 1])\n",
    "                features = np.clip(features, -.99, .99)\n",
    "                \n",
    "                probs_onehot = np.zeros_like(probs)\n",
    "                probs_onehot[np.arange(len(probs)), opt_sel] = 1\n",
    "                \n",
    "                # additional modes are appended to represent categorical component\n",
    "                extra_bits = np.zeros([len(current), len(info['modal'])])\n",
    "                temp_probs_onehot = np.concatenate([extra_bits,probs_onehot], axis = 1)\n",
    "                \n",
    "                # storing the final normalized value and one-hot-encoding of selected modes\n",
    "                final = np.zeros([len(data), 1 + probs_onehot.shape[1] + len(info['modal'])])\n",
    "\n",
    "                # iterates through only the continuous component\n",
    "                features_curser = 0\n",
    "\n",
    "                for idx, val in enumerate(data[:, id_]):\n",
    "                    \n",
    "                    if val in info['modal']:\n",
    "                        # dealing with the modes of categorical component\n",
    "                        category_ = list(map(info['modal'].index, [val]))[0]\n",
    "                        final[idx, 0] = mode_vals[category_]\n",
    "                        final[idx, (category_+1)] = 1\n",
    "                    \n",
    "                    else:\n",
    "                        # dealing with the modes of continuous component\n",
    "                        final[idx, 0] = features[features_curser]\n",
    "                        final[idx, (1+len(info['modal'])):] = temp_probs_onehot[features_curser][len(info['modal']):]\n",
    "                        features_curser = features_curser + 1\n",
    "\n",
    "                # re-ordering the one-hot-encoding of modes in descending order as per their frequency of being selected\n",
    "                just_onehot = final[:,1:]\n",
    "                re_ordered_jhot= np.zeros_like(just_onehot)\n",
    "                n = just_onehot.shape[1]\n",
    "                col_sums = just_onehot.sum(axis=0)\n",
    "                largest_indices = np.argsort(-1*col_sums)[:n]\n",
    "                \n",
    "                for id,val in enumerate(largest_indices):\n",
    "                      re_ordered_jhot[:,id] = just_onehot[:,val]\n",
    "                \n",
    "                final_features = final[:,0].reshape([-1, 1])\n",
    "                \n",
    "                # storing the original ordering for invoking inverse transform\n",
    "                self.ordering.append(largest_indices)\n",
    "                \n",
    "                values += [final_features, re_ordered_jhot]\n",
    "                \n",
    "                mixed_counter = mixed_counter + 1\n",
    "    \n",
    "            else:\n",
    "                # for categorical columns, standard one-hot-encoding is applied where categories are in descending order of frequency by default\n",
    "                self.ordering.append(None)\n",
    "                col_t = np.zeros([len(data), info['size']])\n",
    "                idx = list(map(info['i2s'].index, current))\n",
    "                col_t[np.arange(len(data)), idx] = 1\n",
    "                values.append(col_t)\n",
    "                \n",
    "        return np.concatenate(values, axis=1)\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        \n",
    "        # stores the final inverse transformed generated data \n",
    "        data_t = np.zeros([len(data), len(self.meta)])\n",
    "        \n",
    "        # used to iterate through the columns of the raw generated data\n",
    "        st = 0\n",
    "\n",
    "        # iterating through original column information\n",
    "        for id_, info in enumerate(self.meta):\n",
    "            if info['type'] == \"continuous\":\n",
    "                \n",
    "                # obtaining the generated normalized values and clipping for stability\n",
    "                u = data[:, st]\n",
    "                u = np.clip(u, -1, 1)\n",
    "                \n",
    "                # obtaining the one-hot-encoding of the modes representing the normalized values\n",
    "                v = data[:, st + 1:st + 1 + np.sum(self.components[id_])]\n",
    "                \n",
    "                # re-ordering the modes as per their original ordering\n",
    "                order = self.ordering[id_] \n",
    "                v_re_ordered = np.zeros_like(v)\n",
    "                for id,val in enumerate(order):\n",
    "                    v_re_ordered[:,val] = v[:,id]\n",
    "                v = v_re_ordered\n",
    "\n",
    "                # ensuring un-used modes are represented with -100 such that they can be ignored when computing argmax\n",
    "                v_t = np.ones((data.shape[0], self.n_clusters)) * -100\n",
    "                v_t[:, self.components[id_]] = v\n",
    "                v = v_t\n",
    "                \n",
    "                # obtaining approriate means and stds as per the appropriately selected mode for each data point based on fitted bgm model\n",
    "                means = self.model[id_].means_.reshape([-1])\n",
    "                stds = np.sqrt(self.model[id_].covariances_).reshape([-1])\n",
    "                p_argmax = np.argmax(v, axis=1)\n",
    "                std_t = stds[p_argmax]\n",
    "                mean_t = means[p_argmax]\n",
    "                \n",
    "                # executing the inverse transformation \n",
    "                tmp = u * 4 * std_t + mean_t\n",
    "                \n",
    "                data_t[:, id_] = tmp\n",
    "                \n",
    "                # moving to the next set of columns in the raw generated data in correspondance to original column information\n",
    "                st += 1 + np.sum(self.components[id_])\n",
    "                \n",
    "            elif info['type'] == \"mixed\":\n",
    "                \n",
    "                # obtaining the generated normalized values and corresponding modes\n",
    "                u = data[:, st]\n",
    "                u = np.clip(u, -1, 1)\n",
    "                full_v = data[:,(st+1):(st+1)+len(info['modal'])+np.sum(self.components[id_])]\n",
    "                \n",
    "                # re-ordering the modes as per their original ordering\n",
    "                order = self.ordering[id_]\n",
    "                full_v_re_ordered = np.zeros_like(full_v)\n",
    "                for id,val in enumerate(order):\n",
    "                    full_v_re_ordered[:,val] = full_v[:,id]\n",
    "                full_v = full_v_re_ordered                \n",
    "                \n",
    "                # modes of categorical component\n",
    "                mixed_v = full_v[:,:len(info['modal'])]\n",
    "                \n",
    "                # modes of continuous component\n",
    "                v = full_v[:,-np.sum(self.components[id_]):]\n",
    "\n",
    "                # similarly ensuring un-used modes are represented with -100 to be ignored while computing argmax\n",
    "                v_t = np.ones((data.shape[0], self.n_clusters)) * -100\n",
    "                v_t[:, self.components[id_]] = v\n",
    "                v = np.concatenate([mixed_v,v_t], axis=1)       \n",
    "                p_argmax = np.argmax(v, axis=1)\n",
    "\n",
    "                # obtaining the means and stds of the continuous component using second fitted bgm model\n",
    "                means = self.model[id_][1].means_.reshape([-1]) \n",
    "                stds = np.sqrt(self.model[id_][1].covariances_).reshape([-1]) \n",
    "\n",
    "                # used to store the inverse-transformed data points\n",
    "                result = np.zeros_like(u)\n",
    "\n",
    "                for idx in range(len(data)):\n",
    "                    # in case of categorical mode being selected, the mode value itself is simply assigned \n",
    "                    if p_argmax[idx] < len(info['modal']):\n",
    "                        argmax_value = p_argmax[idx]\n",
    "                        result[idx] = float(list(map(info['modal'].__getitem__, [argmax_value]))[0])\n",
    "                    else:\n",
    "                        # in case of continuous mode being selected, similar inverse-transform for purely numeric values is applied\n",
    "                        std_t = stds[(p_argmax[idx]-len(info['modal']))]\n",
    "                        mean_t = means[(p_argmax[idx]-len(info['modal']))]\n",
    "                        result[idx] = u[idx] * 4 * std_t + mean_t\n",
    "            \n",
    "                data_t[:, id_] = result\n",
    "\n",
    "                st += 1 + np.sum(self.components[id_]) + len(info['modal'])\n",
    "                \n",
    "            else:\n",
    "                # reversing one hot encoding back to label encoding for categorical columns \n",
    "                current = data[:, st:st + info['size']]\n",
    "                idx = np.argmax(current, axis=1)\n",
    "                data_t[:, id_] = list(map(info['i2s'].__getitem__, idx))\n",
    "                st += info['size']\n",
    "        return data_t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b643c80-9e5a-48bd-be20-f266f4ed8a08",
   "metadata": {},
   "source": [
    "# Improving GMM encoding for continuous value column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb67b62-2b49-4cc5-a2eb-a1d3a086dbf3",
   "metadata": {},
   "source": [
    "Following we give an example of how to use our current gaussian mixture model to encode and decode continuous column. For this test, we have three demands:\n",
    "1. Current test is based on a small dataset. Please scale the code to enable GMM encoding on 1B rows data. This part is actually two sub-tasks (i) read 1B rows data into the algorithm and (ii) scale current GMM method to encode 1B rows data within a reasonable time.\n",
    "2. Properly evaluate the new GMM encoder. Make sure all the values can be inverse transformed. Especially the extreme values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524d927f-89ea-42e6-82b8-6c79bc81188c",
   "metadata": {},
   "source": [
    "## Load data and encode column using gaussian mixture method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fc166af-ea89-4844-80a3-fb6e56d718d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"Credit.csv\")[[\"Amount\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f22e0c44-b7bb-4f42-b832-aa45fc5b248a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>197.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Amount\n",
       "0   14.61\n",
       "1    1.00\n",
       "2  197.04\n",
       "3    1.00\n",
       "4   23.25"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e52c78f2-5e51-4548-9d1f-e1ae7660ef35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\mixture\\_base.py:270: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "transformer = DataTransformer(train_data=train_data)\n",
    "transformer.fit() \n",
    "transformed_train_data = transformer.transform(train_data.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725a2f37-2f1b-45f8-abdb-6d212a9d662b",
   "metadata": {},
   "source": [
    "### Show the encoding for value 14.61"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8791b6a0-b607-4e86-b8e1-b87817116d40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.44648112, 1.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        ])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4362ca-7963-4e69-9477-16bf1f06085e",
   "metadata": {},
   "source": [
    "## Inverse transform back the encoded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddee28f2-ff90-460e-b835-b8cb76bc0511",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_transformed_train_data = transformer.inverse_transform(transformed_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15757369-273e-4c0a-aecd-96089bf0571f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 14.61],\n",
       "       [  1.  ],\n",
       "       [197.04],\n",
       "       ...,\n",
       "       [ 12.  ],\n",
       "       [ 36.  ],\n",
       "       [108.  ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverse_transformed_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbee1f32-f571-4192-a197-4d83fd004fff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>197.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Amount\n",
       "0   14.61\n",
       "1    1.00\n",
       "2  197.04\n",
       "3    1.00\n",
       "4   23.25"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(inverse_transformed_train_data, columns=[\"Amount\"]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a8e6d4-562b-45b0-8b70-716f0cd0bc2d",
   "metadata": {},
   "source": [
    "# Scale code to handle 1B rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11ad3981-0e57-47b6-ade5-c87b259a345a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4dc52722-6951-4435-a839-4cbe85683f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Dask client with specific worker configurations\n",
    "client = Client(n_workers=2, threads_per_worker=1, memory_limit='4GB')  # Adjust memory allocation as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a489d72b-f53d-4ad9-a082-38ec3b79321e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dask.config.set at 0x21ac01c29d0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configure Dask's memory management settings\n",
    "dask.config.set({\n",
    "    \"distributed.worker.memory.spill\": 0.7,   # Spill data to disk at 70% memory usage\n",
    "    \"distributed.worker.memory.pause\": 0.8,   # Pause task submission at 80% memory usage\n",
    "    \"distributed.worker.memory.terminate\": 0.95  # Terminate worker at 95% to prevent crashes\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4fc21e87-bfbb-4ba1-823b-42b16e469697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the DataTransformer class with VGM-based normalization and validation\n",
    "class DataTransformer:\n",
    "    def __init__(self, train_data, categorical_list=[], mixed_dict={}, n_clusters=10, n_components=3, eps=0.005):\n",
    "        if train_data.ndim == 1:\n",
    "            train_data = train_data.reshape(-1, 1)\n",
    "        self.train_data = train_data\n",
    "        self.vgm_params = {}\n",
    "        self.n_components = n_components\n",
    "        self.categorical_columns = categorical_list\n",
    "        self.mixed_columns = mixed_dict\n",
    "        self.components = []\n",
    "        self.n_clusters = n_clusters\n",
    "        self.eps = eps\n",
    "        self.meta = self.get_metadata()\n",
    "        self.ordering = []\n",
    "        self.output_info = []\n",
    "        self.output_dim = 0\n",
    "        self.components = []\n",
    "        self.filter_arr = []\n",
    "    \n",
    "    def get_metadata(self):\n",
    "        meta = []\n",
    "        for index in range(self.train_data.shape[1]):\n",
    "            column = self.train_data[:, index]\n",
    "            if index in self.categorical_columns:\n",
    "                mapper = pd.Series(column).value_counts().index.tolist()\n",
    "                meta.append({\n",
    "                    \"name\": index,\n",
    "                    \"type\": \"categorical\",\n",
    "                    \"size\": len(mapper),\n",
    "                    \"i2s\": mapper  # index to string mapping for inverse transform\n",
    "                })\n",
    "            elif index in self.mixed_columns.keys():\n",
    "                meta.append({\n",
    "                    \"name\": index,\n",
    "                    \"type\": \"mixed\",\n",
    "                    \"min\": np.min(column),\n",
    "                    \"max\": np.max(column),\n",
    "                    \"modal\": self.mixed_columns[index]\n",
    "                })\n",
    "            else:\n",
    "                meta.append({\n",
    "                    \"name\": index,\n",
    "                    \"type\": \"continuous\",\n",
    "                    \"min\": np.min(column),\n",
    "                    \"max\": np.max(column),\n",
    "                })\n",
    "        return meta\n",
    "\n",
    "    def fit(self):\n",
    "        data = self.train_data\n",
    "        model = []\n",
    "    \n",
    "        for id_, info in enumerate(self.meta):\n",
    "            if info['type'] == \"continuous\":\n",
    "                gm = BayesianGaussianMixture(\n",
    "                    n_components=self.n_clusters,\n",
    "                    weight_concentration_prior_type='dirichlet_process',\n",
    "                    weight_concentration_prior=0.001,\n",
    "                    max_iter=100, n_init=1, random_state=42\n",
    "                )\n",
    "                gm.fit(data[:, id_].reshape([-1, 1]))\n",
    "                model.append(gm)\n",
    "    \n",
    "                old_comp = gm.weights_ > self.eps\n",
    "                mode_freq = pd.Series(gm.predict(data[:, id_].reshape([-1, 1]))).value_counts().keys()\n",
    "                comp = [i in mode_freq and old_comp[i] for i in range(self.n_clusters)]\n",
    "                self.components.append(comp)\n",
    "    \n",
    "                # Populate ordering only with valid indices\n",
    "                order = np.argsort(-1 * mode_freq)\n",
    "                valid_order = [i for i in order if i < np.sum(comp) and i < self.n_clusters]\n",
    "                self.ordering.append(valid_order)\n",
    "    \n",
    "                self.output_info += [(1, 'tanh'), (np.sum(comp), 'softmax')]\n",
    "                self.output_dim += 1 + np.sum(comp)\n",
    "    \n",
    "            elif info['type'] == \"mixed\":\n",
    "                # Handle mixed types if necessary\n",
    "                pass\n",
    "    \n",
    "            else:\n",
    "                model.append(None)\n",
    "                self.components.append(None)\n",
    "                self.ordering.append(None)\n",
    "                self.output_info += [(info['size'], 'softmax')]\n",
    "                self.output_dim += info['size']\n",
    "    \n",
    "        self.model = model\n",
    "        logging.info(f\"Ordering after fitting: {self.ordering}\")\n",
    "\n",
    "\n",
    "    def transform(self, data):\n",
    "        # Ensure the data is reshaped to 2D for vectorized operations\n",
    "        if data.ndim == 1:\n",
    "            data = data.reshape(-1, 1)\n",
    "    \n",
    "        values = []  # List to store transformed data\n",
    "        for id_, info in enumerate(self.meta):\n",
    "            current = data[:, id_]\n",
    "            \n",
    "            if info['type'] == 'continuous':\n",
    "                current = current.reshape(-1, 1)\n",
    "                \n",
    "                # Vectorized mode-specific normalization\n",
    "                means = self.model[id_].means_.reshape((1, self.n_clusters))\n",
    "                stds = np.sqrt(self.model[id_].covariances_).reshape((1, self.n_clusters))\n",
    "                features = (current - means) / (4 * stds)\n",
    "                \n",
    "                # Get valid components and probabilities\n",
    "                n_opts = np.sum(self.components[id_])\n",
    "                opt_sel = np.zeros(len(data), dtype=int)\n",
    "                probs = self.model[id_].predict_proba(current)\n",
    "                probs = probs[:, self.components[id_]]\n",
    "                \n",
    "                for i in range(len(data)):\n",
    "                    pp = probs[i] + 1e-6\n",
    "                    pp /= np.sum(pp)\n",
    "                    opt_sel[i] = np.random.choice(np.arange(n_opts), p=pp)\n",
    "                \n",
    "                idx = np.arange(len(features))\n",
    "                features = features[:, self.components[id_]]\n",
    "                features = features[idx, opt_sel].reshape(-1, 1)\n",
    "                features = np.clip(features, -0.99, 0.99)\n",
    "                \n",
    "                onehot_encoded = np.zeros_like(probs)\n",
    "                onehot_encoded[np.arange(len(probs)), opt_sel] = 1\n",
    "                \n",
    "                values.append(features)\n",
    "                values.append(onehot_encoded)\n",
    "            \n",
    "            elif info['type'] == 'categorical':\n",
    "                col_t = np.zeros((len(data), info['size']))\n",
    "                idx = list(map(info['i2s'].index, current))\n",
    "                col_t[np.arange(len(data)), idx] = 1\n",
    "                values.append(col_t)\n",
    "    \n",
    "        # Combine all parts of transformed data\n",
    "        transformed_data = np.concatenate(values, axis=1)\n",
    "        logging.info(f\"Transformed data shape: {transformed_data.shape}, Expected output_dim: {self.output_dim}\")\n",
    "    \n",
    "        if transformed_data.shape[1] != self.output_dim:\n",
    "            raise ValueError(\"Transformed data does not match the expected output dimension.\")\n",
    "        \n",
    "        return transformed_data\n",
    "\n",
    "\n",
    "    def inverse_transform(self, transformed_data):\n",
    "        if transformed_data.ndim == 1:\n",
    "            transformed_data = transformed_data.reshape(-1, 1)\n",
    "    \n",
    "        data_t = np.zeros((len(transformed_data), len(self.meta)))\n",
    "        start_col = 0\n",
    "    \n",
    "        for id_, info in enumerate(self.meta):\n",
    "            if info['type'] == \"continuous\":\n",
    "                if id_ >= len(self.ordering) or self.ordering[id_] is None:\n",
    "                    logging.error(f\"Invalid or missing ordering at index {id_}\")\n",
    "                    raise IndexError(\"Invalid or missing ordering during inverse transformation.\")\n",
    "    \n",
    "                u = transformed_data[:, start_col]\n",
    "                u = np.clip(u, -1, 1)\n",
    "    \n",
    "                end_col = start_col + 1 + np.sum(self.components[id_])\n",
    "                v = transformed_data[:, start_col + 1:end_col]\n",
    "    \n",
    "                # Ensure `order` indices are within bounds\n",
    "                order = self.ordering[id_]\n",
    "                v_reordered = np.zeros_like(v)\n",
    "                for i, val in enumerate(order):\n",
    "                    if val >= v.shape[1]:\n",
    "                        logging.error(f\"Order index {val} out of bounds for v shape {v.shape}\")\n",
    "                        continue  # Skip out-of-bounds indices to prevent errors\n",
    "                    v_reordered[:, val] = v[:, i]\n",
    "                v = v_reordered\n",
    "    \n",
    "                v_full = np.ones((len(transformed_data), self.n_clusters)) * -100\n",
    "                v_full[:, self.components[id_]] = v\n",
    "                v = v_full\n",
    "    \n",
    "                p_argmax = np.argmax(v, axis=1)\n",
    "                means = self.model[id_].means_.reshape(-1)\n",
    "                stds = np.sqrt(self.model[id_].covariances_).reshape(-1)\n",
    "                mean_t = means[p_argmax]\n",
    "                std_t = stds[p_argmax]\n",
    "    \n",
    "                original_values = u * 4 * std_t + mean_t\n",
    "                data_t[:, id_] = original_values\n",
    "    \n",
    "                start_col = end_col\n",
    "    \n",
    "            elif info['type'] == \"categorical\":\n",
    "                current = transformed_data[:, start_col:start_col + info['size']]\n",
    "                idx = np.argmax(current, axis=1)\n",
    "                data_t[:, id_] = list(map(info['i2s'].__getitem__, idx))\n",
    "                start_col += info['size']\n",
    "    \n",
    "            # Additional logic for mixed types if needed\n",
    "    \n",
    "        return data_t\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "157d9281-4b12-4472-957b-f5d709654ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to transform data chunks with timing and logging\n",
    "def transform_chunk_with_timing(chunk, transformer):\n",
    "    start_time = time.time()\n",
    "    logging.info(\"Starting transformation of a chunk.\")\n",
    "    \n",
    "    # Transform the data and validate\n",
    "    transformed_chunk = transformer.transform(chunk.values)\n",
    "    \n",
    "    # Log time taken and validate the transformed data\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Chunk transformed in {elapsed_time:.2f} seconds.\")\n",
    "    \n",
    "    return transformed_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b358871f-128b-4a0e-95a3-1b93bb3c7a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to trigger garbage collection and log memory usage\n",
    "def manual_gc():\n",
    "    gc.collect()\n",
    "    logging.info(\"Garbage collection triggered.\")\n",
    "def log_memory_usage():\n",
    "    mem = psutil.virtual_memory()\n",
    "    logging.info(f\"Current memory usage: {mem.percent}% of total {mem.total / (1024 ** 3):.2f} GB.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6cbcc543-a365-4d55-9ee3-a9e5e3ee29ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999980046 rows are loaded.\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV using Dask with an optimized block size\n",
    "chunk_size = \"100MB\"  # Adjust to a smaller value to prevent high memory usage\n",
    "dask_df = dd.read_csv(\"Credit.csv\", usecols=[\"Amount\"], blocksize=chunk_size)\n",
    "\n",
    "# Convert data types to reduce memory usage\n",
    "dask_df[\"Amount\"] = dask_df[\"Amount\"].astype(np.float32)\n",
    "\n",
    "# Scale the data to simulate larger datasets (e.g., 1 billion rows)\n",
    "scaled_dask_df = dd.concat([dask_df] * (1_000_000_000 // len(dask_df)), interleave_partitions=True)\n",
    "print(f\"{len(scaled_dask_df)} rows are loaded.\")\n",
    "\n",
    "# Repartition data to manage memory better\n",
    "scaled_dask_df = scaled_dask_df.repartition(npartitions=500)  # Adjust the number of partitions as needed\n",
    "\n",
    "# Persist data to prevent recomputation and manage memory\n",
    "scaled_dask_df = scaled_dask_df.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0802cc9d-8e08-4a75-b1e7-aa70cd03a296",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-11 03:12:45,187 - Starting transformation of a chunk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGM parameters fitted.\n",
      "Chunk transformed in 0.00 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-11 03:12:46,327 - Garbage collection triggered.\n",
      "2024-11-11 03:12:46,334 - Current memory usage: 88.4% of total 15.74 GB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Processing Time for All Chunks: 1.16 seconds\n"
     ]
    }
   ],
   "source": [
    "# Initialize and fit the transformer\n",
    "sample_df = dask_df.head(5_000_000, compute=True)  # Use a sample to fit the VGM\n",
    "transformer = DataTransformer(train_data=sample_df['Amount'].values)\n",
    "transformer.fit()\n",
    "\n",
    "# Run the transformation in parallel using Dask with timing and validation\n",
    "start_total_time = time.time()\n",
    "try:\n",
    "    transformed_chunks = dask_df.map_partitions(\n",
    "        lambda df: transform_chunk_with_timing(df['Amount'], transformer)\n",
    "    ).compute()\n",
    "    \n",
    "    # Trigger garbage collection after processing\n",
    "    manual_gc()\n",
    "    log_memory_usage()\n",
    "\n",
    "except ValueError as e:\n",
    "    print(f\"Error during transformation: {e}\")\n",
    "    client.shutdown()  # Gracefully shut down if an error occurs\n",
    "    raise\n",
    "\n",
    "total_time = time.time() - start_total_time\n",
    "print(f\"Total Processing Time for All Chunks: {total_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288554c6-a513-4f07-b1a5-2d8a62246ca5",
   "metadata": {},
   "source": [
    "# Validation on Extreme Values and inverse transformation evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "28e75e42-3767-4e9e-88a6-b7894c69b8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to validate extreme values in the transformed data\n",
    "def validate_extreme_values(transformed_data):\n",
    "    max_value = np.max(transformed_data)\n",
    "    min_value = np.min(transformed_data)\n",
    "    if max_value > 1 or min_value < -1:\n",
    "        logging.warning(f\"Extreme values detected: max={max_value}, min={min_value}.\")\n",
    "    else:\n",
    "        logging.info(f\"No extreme values detected: max={max_value}, min={min_value}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "de3888b8-fd37-43a9-a591-b244d2764a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_gmm_encoder(transformer, test_data):\n",
    "    logging.info(\"Starting data transformation for evaluation.\")\n",
    "    transformed_data = transformer.transform(test_data)\n",
    "\n",
    "    # Validate extreme values\n",
    "    validate_extreme_values(transformed_data)\n",
    "\n",
    "    # Log the shape and output_dim for verification\n",
    "    logging.info(f\"Shape of transformed_data: {transformed_data.shape}\")\n",
    "    logging.info(f\"Expected output_dim: {transformer.output_dim}\")\n",
    "\n",
    "    if transformed_data.shape[1] != transformer.output_dim:\n",
    "        logging.error(\"Mismatch between transformed_data shape and expected output_dim.\")\n",
    "        raise ValueError(\"Transformed data does not match the expected output dimension.\")\n",
    "\n",
    "    # Attempt inverse transformation\n",
    "    try:\n",
    "        logging.info(\"Evaluating inverse transformation.\")\n",
    "        inverse_data = transformer.inverse_transform(transformed_data)\n",
    "\n",
    "        # Check for consistency\n",
    "        differences = np.abs(test_data - inverse_data)\n",
    "        max_diff = np.max(differences)\n",
    "        if max_diff > 1e-5:  # Tolerance level for floating-point precision\n",
    "            logging.warning(f\"Inverse transformation inconsistency detected. Max difference: {max_diff:.5f}\")\n",
    "        else:\n",
    "            logging.info(\"Inverse transformation is consistent within the acceptable tolerance.\")\n",
    "        return inverse_data\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during inverse transformation: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8b0c890b-a677-48fd-bef8-9ca4f7440f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test dataset that includes typical and extreme values\n",
    "def create_test_data(original_data, num_samples=1000):\n",
    "    # Generate some extreme values based on the original data's range\n",
    "    max_val = np.max(original_data)\n",
    "    min_val = np.min(original_data)\n",
    "\n",
    "    # Create a dataset with typical values, and add extreme values for testing\n",
    "    test_data = np.concatenate([\n",
    "        original_data[:num_samples],  # Sample of the original data\n",
    "        np.linspace(min_val, max_val, num_samples // 2),  # Linearly spaced extreme values\n",
    "        np.array([min_val - 10, max_val + 10])  # Out-of-range extreme values\n",
    "    ])\n",
    "    return test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "54bf7e47-d6dd-434d-a23f-9a95f6d32a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\mixture\\_base.py:270: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
      "  warnings.warn(\n",
      "2024-11-11 03:57:48,700 - Ordering after fitting: [[5, 1, 6, 3, 2, 4, 0]]\n",
      "2024-11-11 03:57:48,701 - Starting data transformation for evaluation.\n",
      "2024-11-11 03:57:48,754 - Transformed data shape: (1502, 8), Expected output_dim: 8\n",
      "2024-11-11 03:57:48,755 - No extreme values detected: max=1.0, min=-0.6815188384091836.\n",
      "2024-11-11 03:57:48,755 - Shape of transformed_data: (1502, 8)\n",
      "2024-11-11 03:57:48,755 - Expected output_dim: 8\n",
      "2024-11-11 03:57:48,756 - Evaluating inverse transformation.\n",
      "2024-11-11 03:57:48,768 - Inverse transformation inconsistency detected. Max difference: 11804.41351\n",
      "2024-11-11 03:57:48,770 - Sample of inverse transformed data:\n",
      "[[1035.44925468]\n",
      " [  13.4294387 ]\n",
      " [ 197.03999329]\n",
      " [  13.4294387 ]\n",
      " [ 450.75828248]]\n"
     ]
    }
   ],
   "source": [
    "# Run the evaluation with the new inverse_transform method included\n",
    "sample_size = 100000  # Adjust as needed\n",
    "sample_df = scaled_dask_df.head(sample_size, compute=True)\n",
    "transformer = DataTransformer(train_data=sample_df['Amount'].values)\n",
    "transformer.fit()\n",
    "\n",
    "# Create test data for evaluation\n",
    "test_data = create_test_data(sample_df['Amount'].values)\n",
    "\n",
    "# Evaluate GMM encoder and inverse transformation\n",
    "inverse_data = evaluate_gmm_encoder(transformer, test_data)\n",
    "\n",
    "# Print a sample of the inverse-transformed data\n",
    "logging.info(f\"Sample of inverse transformed data:\\n{inverse_data[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d9c77b-92d5-4076-bab5-5212b5726686",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
