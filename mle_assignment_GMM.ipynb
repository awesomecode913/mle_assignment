{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ddaf923-f7eb-408c-ae66-4c446411e034",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "import time\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "195c4b4e-cf62-4ce9-9ed9-d1e34c85ceba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformer():\n",
    "    \n",
    "    \"\"\"\n",
    "    Transformer class responsible for processing data to train the CTABGANSynthesizer model\n",
    "    \n",
    "    Variables:\n",
    "    1) train_data -> input dataframe \n",
    "    2) categorical_list -> list of categorical columns\n",
    "    3) mixed_dict -> dictionary of mixed columns\n",
    "    4) n_clusters -> number of modes to fit bayesian gaussian mixture (bgm) model\n",
    "    5) eps -> threshold for ignoring less prominent modes in the mixture model \n",
    "    6) ordering -> stores original ordering for modes of numeric columns\n",
    "    7) output_info -> stores dimension and output activations of columns (i.e., tanh for numeric, softmax for categorical)\n",
    "    8) output_dim -> stores the final column width of the transformed data\n",
    "    9) components -> stores the valid modes used by numeric columns\n",
    "    10) filter_arr -> stores valid indices of continuous component in mixed columns\n",
    "    11) meta -> stores column information corresponding to different data types i.e., categorical/mixed/numerical\n",
    "\n",
    "\n",
    "    Methods:\n",
    "    1) __init__() -> initializes transformer object and computes meta information of columns\n",
    "    2) get_metadata() -> builds an inventory of individual columns and stores their relevant properties\n",
    "    3) fit() -> fits the required bgm models to process the input data\n",
    "    4) transform() -> executes the transformation required to train the model\n",
    "    5) inverse_transform() -> executes the reverse transformation on data generated from the model\n",
    "    \n",
    "    \"\"\"    \n",
    "    \n",
    "    def __init__(self, train_data=pd.DataFrame, categorical_list=[], mixed_dict={}, n_clusters=10, eps=0.005):\n",
    "        \n",
    "        self.meta = None\n",
    "        self.train_data = train_data\n",
    "        self.categorical_columns= categorical_list\n",
    "        self.mixed_columns= mixed_dict\n",
    "        self.n_clusters = n_clusters\n",
    "        self.eps = eps\n",
    "        self.ordering = []\n",
    "        self.output_info = []\n",
    "        self.output_dim = 0\n",
    "        self.components = []\n",
    "        self.filter_arr = []\n",
    "        self.meta = self.get_metadata()\n",
    "        \n",
    "    def get_metadata(self):\n",
    "        \n",
    "        meta = []\n",
    "    \n",
    "        for index in range(self.train_data.shape[1]):\n",
    "            column = self.train_data.iloc[:,index]\n",
    "            if index in self.categorical_columns:\n",
    "                mapper = column.value_counts().index.tolist()\n",
    "                meta.append({\n",
    "                        \"name\": index,\n",
    "                        \"type\": \"categorical\",\n",
    "                        \"size\": len(mapper),\n",
    "                        \"i2s\": mapper\n",
    "                })\n",
    "            elif index in self.mixed_columns.keys():\n",
    "                meta.append({\n",
    "                    \"name\": index,\n",
    "                    \"type\": \"mixed\",\n",
    "                    \"min\": column.min(),\n",
    "                    \"max\": column.max(),\n",
    "                    \"modal\": self.mixed_columns[index]\n",
    "                })\n",
    "            else:\n",
    "                meta.append({\n",
    "                    \"name\": index,\n",
    "                    \"type\": \"continuous\",\n",
    "                    \"min\": column.min(),\n",
    "                    \"max\": column.max(),\n",
    "                })            \n",
    "\n",
    "        return meta\n",
    "\n",
    "    def fit(self):\n",
    "        \n",
    "        data = self.train_data.values\n",
    "        \n",
    "        # stores the corresponding bgm models for processing numeric data\n",
    "        model = []\n",
    "        \n",
    "        # iterating through column information\n",
    "        for id_, info in enumerate(self.meta):\n",
    "            if info['type'] == \"continuous\":\n",
    "                # fitting bgm model  \n",
    "                gm = BayesianGaussianMixture(\n",
    "                    n_components = self.n_clusters, \n",
    "                    weight_concentration_prior_type='dirichlet_process',\n",
    "                    weight_concentration_prior=0.001, # lower values result in lesser modes being active\n",
    "                    max_iter=100,n_init=1, random_state=42)\n",
    "                gm.fit(data[:, id_].reshape([-1, 1]))\n",
    "                model.append(gm)\n",
    "                # keeping only relevant modes that have higher weight than eps and are used to fit the data\n",
    "                old_comp = gm.weights_ > self.eps\n",
    "                mode_freq = (pd.Series(gm.predict(data[:, id_].reshape([-1, 1]))).value_counts().keys())\n",
    "                comp = []\n",
    "                for i in range(self.n_clusters):\n",
    "                    if (i in (mode_freq)) & old_comp[i]:\n",
    "                        comp.append(True)\n",
    "                    else:\n",
    "                        comp.append(False)\n",
    "                self.components.append(comp) \n",
    "                self.output_info += [(1, 'tanh'), (np.sum(comp), 'softmax')]\n",
    "                self.output_dim += 1 + np.sum(comp)\n",
    "                \n",
    "            elif info['type'] == \"mixed\":\n",
    "                \n",
    "                # in case of mixed columns, two bgm models are used\n",
    "                gm1 = BayesianGaussianMixture(\n",
    "                    self.n_clusters, \n",
    "                    weight_concentration_prior_type='dirichlet_process',\n",
    "                    weight_concentration_prior=0.001, max_iter=100,\n",
    "                    n_init=1,random_state=42)\n",
    "                gm2 = BayesianGaussianMixture(\n",
    "                    self.n_clusters,\n",
    "                    weight_concentration_prior_type='dirichlet_process',\n",
    "                    weight_concentration_prior=0.001, max_iter=100,\n",
    "                    n_init=1,random_state=42)\n",
    "                \n",
    "                # first bgm model is fit to the entire data only for the purposes of obtaining a normalized value of any particular categorical mode\n",
    "                gm1.fit(data[:, id_].reshape([-1, 1]))\n",
    "                \n",
    "                # main bgm model used to fit the continuous component and serves the same purpose as with purely numeric columns\n",
    "                filter_arr = []\n",
    "                for element in data[:, id_]:\n",
    "                    if element not in info['modal']:\n",
    "                        filter_arr.append(True)\n",
    "                    else:\n",
    "                        filter_arr.append(False)\n",
    "                self.filter_arr.append(filter_arr)\n",
    "                \n",
    "                gm2.fit(data[:, id_][filter_arr].reshape([-1, 1]))\n",
    "                \n",
    "                model.append((gm1,gm2))\n",
    "                \n",
    "                # similarly keeping only relevant modes with higher weight than eps and are used to fit strictly continuous data \n",
    "                old_comp = gm2.weights_ > self.eps\n",
    "                mode_freq = (pd.Series(gm2.predict(data[:, id_][filter_arr].reshape([-1, 1]))).value_counts().keys())  \n",
    "                comp = []\n",
    "                  \n",
    "                for i in range(self.n_clusters):\n",
    "                    if (i in (mode_freq)) & old_comp[i]:\n",
    "                        comp.append(True)\n",
    "                    else:\n",
    "                        comp.append(False)\n",
    "\n",
    "                self.components.append(comp)\n",
    "                \n",
    "                # modes of the categorical component are appended to modes produced by the main bgm model\n",
    "                self.output_info += [(1, 'tanh'), (np.sum(comp) + len(info['modal']), 'softmax')]\n",
    "                self.output_dim += 1 + np.sum(comp) + len(info['modal'])\n",
    "            \n",
    "            else:\n",
    "                # in case of categorical columns, bgm model is ignored\n",
    "                model.append(None)\n",
    "                self.components.append(None)\n",
    "                self.output_info += [(info['size'], 'softmax')]\n",
    "                self.output_dim += info['size']\n",
    "        \n",
    "        self.model = model\n",
    "\n",
    "    def transform(self, data):\n",
    "        \n",
    "        # stores the transformed values\n",
    "        values = []\n",
    "\n",
    "        # used for accessing filter_arr for transforming mixed columns\n",
    "        mixed_counter = 0\n",
    "        \n",
    "        # iterating through column information\n",
    "        for id_, info in enumerate(self.meta):\n",
    "            current = data[:, id_]\n",
    "            if info['type'] == \"continuous\":\n",
    "                # mode-specific normalization occurs here\n",
    "                current = current.reshape([-1, 1])\n",
    "                # means and stds of the modes are obtained from the corresponding fitted bgm model\n",
    "                means = self.model[id_].means_.reshape((1, self.n_clusters))\n",
    "                stds = np.sqrt(self.model[id_].covariances_).reshape((1, self.n_clusters))\n",
    "                # values are then normalized and stored for all modes\n",
    "                features = np.empty(shape=(len(current),self.n_clusters))\n",
    "                # note 4 is a multiplier to ensure values lie between -1 to 1 but this is not always guaranteed\n",
    "                features = (current - means) / (4 * stds) \n",
    "\n",
    "                # number of distict modes\n",
    "                n_opts = sum(self.components[id_])                \n",
    "                # storing the mode for each data point by sampling from the probability mass distribution across all modes based on fitted bgm model \n",
    "                opt_sel = np.zeros(len(data), dtype='int')\n",
    "                probs = self.model[id_].predict_proba(current.reshape([-1, 1]))\n",
    "                probs = probs[:, self.components[id_]]\n",
    "                for i in range(len(data)):\n",
    "                    pp = probs[i] + 1e-6\n",
    "                    pp = pp / sum(pp)\n",
    "                    opt_sel[i] = np.random.choice(np.arange(n_opts), p=pp)\n",
    "                \n",
    "                # creating a one-hot-encoding for the corresponding selected modes\n",
    "                probs_onehot = np.zeros_like(probs)\n",
    "                probs_onehot[np.arange(len(probs)), opt_sel] = 1\n",
    "\n",
    "                # obtaining the normalized values based on the appropriately selected mode and clipping to ensure values are within (-1,1)\n",
    "                idx = np.arange((len(features)))\n",
    "                features = features[:, self.components[id_]]\n",
    "                features = features[idx, opt_sel].reshape([-1, 1])\n",
    "                features = np.clip(features, -.99, .99) \n",
    "                \n",
    "                # re-ordering the one-hot-encoding of modes in descending order as per their frequency of being selected\n",
    "                re_ordered_phot = np.zeros_like(probs_onehot)  \n",
    "                col_sums = probs_onehot.sum(axis=0)\n",
    "                n = probs_onehot.shape[1]\n",
    "                largest_indices = np.argsort(-1*col_sums)[:n]\n",
    "                for id,val in enumerate(largest_indices):\n",
    "                    re_ordered_phot[:,id] = probs_onehot[:,val]\n",
    "                \n",
    "                # storing the original ordering for invoking inverse transform\n",
    "                self.ordering.append(largest_indices)\n",
    "                \n",
    "                # storing transformed numeric column represented as normalized values and corresponding modes \n",
    "                values += [features, re_ordered_phot]\n",
    "                  \n",
    "            elif info['type'] == \"mixed\":\n",
    "                \n",
    "                # means and standard deviation of modes obtained from the first fitted bgm model\n",
    "                means_0 = self.model[id_][0].means_.reshape([-1])\n",
    "                stds_0 = np.sqrt(self.model[id_][0].covariances_).reshape([-1])\n",
    "\n",
    "                # list to store relevant bgm modes for categorical components\n",
    "                zero_std_list = []\n",
    "                \n",
    "                # means and stds needed to normalize relevant categorical components\n",
    "                means_needed = []\n",
    "                stds_needed = []\n",
    "\n",
    "                # obtaining the closest bgm mode to the categorical component\n",
    "                for mode in info['modal']:\n",
    "                    # skipped for mode representing missing values\n",
    "                    if mode!=-9999999:\n",
    "                        dist = []\n",
    "                        for idx,val in enumerate(list(means_0.flatten())):\n",
    "                            dist.append(abs(mode-val))\n",
    "                        index_min = np.argmin(np.array(dist))\n",
    "                        zero_std_list.append(index_min)\n",
    "                    else: continue\n",
    "\n",
    "                \n",
    "                # stores the appropriate normalized value of categorical modes\n",
    "                mode_vals = []\n",
    "                \n",
    "                # based on the means and stds of the chosen modes for categorical components, their respective values are similarly normalized\n",
    "                for idx in zero_std_list:\n",
    "                    means_needed.append(means_0[idx])\n",
    "                    stds_needed.append(stds_0[idx])\n",
    "               \n",
    "                for i,j,k in zip(info['modal'],means_needed,stds_needed):\n",
    "                    this_val  = np.clip(((i - j) / (4*k)), -.99, .99) \n",
    "                    mode_vals.append(this_val)\n",
    "                \n",
    "                # for categorical modes representing missing values, the normalized value associated is simply 0\n",
    "                if -9999999 in info[\"modal\"]:\n",
    "                    mode_vals.append(0)\n",
    "                \n",
    "                # transforming continuous component of mixed columns similar to purely numeric columns using second fitted bgm model\n",
    "                current = current.reshape([-1, 1])\n",
    "                filter_arr = self.filter_arr[mixed_counter]\n",
    "                current = current[filter_arr]\n",
    "    \n",
    "                means = self.model[id_][1].means_.reshape((1, self.n_clusters))\n",
    "                stds = np.sqrt(self.model[id_][1].covariances_).reshape((1, self.n_clusters))\n",
    "                \n",
    "                features = np.empty(shape=(len(current),self.n_clusters))\n",
    "                features = (current - means) / (4 * stds)\n",
    "                \n",
    "                n_opts = sum(self.components[id_]) \n",
    "                probs = self.model[id_][1].predict_proba(current.reshape([-1, 1]))\n",
    "                probs = probs[:, self.components[id_]]\n",
    "                \n",
    "                opt_sel = np.zeros(len(current), dtype='int')\n",
    "                for i in range(len(current)):\n",
    "                    pp = probs[i] + 1e-6\n",
    "                    pp = pp / sum(pp)\n",
    "                    opt_sel[i] = np.random.choice(np.arange(n_opts), p=pp)\n",
    "                \n",
    "                idx = np.arange((len(features)))\n",
    "                features = features[:, self.components[id_]]\n",
    "                features = features[idx, opt_sel].reshape([-1, 1])\n",
    "                features = np.clip(features, -.99, .99)\n",
    "                \n",
    "                probs_onehot = np.zeros_like(probs)\n",
    "                probs_onehot[np.arange(len(probs)), opt_sel] = 1\n",
    "                \n",
    "                # additional modes are appended to represent categorical component\n",
    "                extra_bits = np.zeros([len(current), len(info['modal'])])\n",
    "                temp_probs_onehot = np.concatenate([extra_bits,probs_onehot], axis = 1)\n",
    "                \n",
    "                # storing the final normalized value and one-hot-encoding of selected modes\n",
    "                final = np.zeros([len(data), 1 + probs_onehot.shape[1] + len(info['modal'])])\n",
    "\n",
    "                # iterates through only the continuous component\n",
    "                features_curser = 0\n",
    "\n",
    "                for idx, val in enumerate(data[:, id_]):\n",
    "                    \n",
    "                    if val in info['modal']:\n",
    "                        # dealing with the modes of categorical component\n",
    "                        category_ = list(map(info['modal'].index, [val]))[0]\n",
    "                        final[idx, 0] = mode_vals[category_]\n",
    "                        final[idx, (category_+1)] = 1\n",
    "                    \n",
    "                    else:\n",
    "                        # dealing with the modes of continuous component\n",
    "                        final[idx, 0] = features[features_curser]\n",
    "                        final[idx, (1+len(info['modal'])):] = temp_probs_onehot[features_curser][len(info['modal']):]\n",
    "                        features_curser = features_curser + 1\n",
    "\n",
    "                # re-ordering the one-hot-encoding of modes in descending order as per their frequency of being selected\n",
    "                just_onehot = final[:,1:]\n",
    "                re_ordered_jhot= np.zeros_like(just_onehot)\n",
    "                n = just_onehot.shape[1]\n",
    "                col_sums = just_onehot.sum(axis=0)\n",
    "                largest_indices = np.argsort(-1*col_sums)[:n]\n",
    "                \n",
    "                for id,val in enumerate(largest_indices):\n",
    "                      re_ordered_jhot[:,id] = just_onehot[:,val]\n",
    "                \n",
    "                final_features = final[:,0].reshape([-1, 1])\n",
    "                \n",
    "                # storing the original ordering for invoking inverse transform\n",
    "                self.ordering.append(largest_indices)\n",
    "                \n",
    "                values += [final_features, re_ordered_jhot]\n",
    "                \n",
    "                mixed_counter = mixed_counter + 1\n",
    "    \n",
    "            else:\n",
    "                # for categorical columns, standard one-hot-encoding is applied where categories are in descending order of frequency by default\n",
    "                self.ordering.append(None)\n",
    "                col_t = np.zeros([len(data), info['size']])\n",
    "                idx = list(map(info['i2s'].index, current))\n",
    "                col_t[np.arange(len(data)), idx] = 1\n",
    "                values.append(col_t)\n",
    "                \n",
    "        return np.concatenate(values, axis=1)\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        \n",
    "        # stores the final inverse transformed generated data \n",
    "        data_t = np.zeros([len(data), len(self.meta)])\n",
    "        \n",
    "        # used to iterate through the columns of the raw generated data\n",
    "        st = 0\n",
    "\n",
    "        # iterating through original column information\n",
    "        for id_, info in enumerate(self.meta):\n",
    "            if info['type'] == \"continuous\":\n",
    "                \n",
    "                # obtaining the generated normalized values and clipping for stability\n",
    "                u = data[:, st]\n",
    "                u = np.clip(u, -1, 1)\n",
    "                \n",
    "                # obtaining the one-hot-encoding of the modes representing the normalized values\n",
    "                v = data[:, st + 1:st + 1 + np.sum(self.components[id_])]\n",
    "                \n",
    "                # re-ordering the modes as per their original ordering\n",
    "                order = self.ordering[id_] \n",
    "                v_re_ordered = np.zeros_like(v)\n",
    "                for id,val in enumerate(order):\n",
    "                    v_re_ordered[:,val] = v[:,id]\n",
    "                v = v_re_ordered\n",
    "\n",
    "                # ensuring un-used modes are represented with -100 such that they can be ignored when computing argmax\n",
    "                v_t = np.ones((data.shape[0], self.n_clusters)) * -100\n",
    "                v_t[:, self.components[id_]] = v\n",
    "                v = v_t\n",
    "                \n",
    "                # obtaining approriate means and stds as per the appropriately selected mode for each data point based on fitted bgm model\n",
    "                means = self.model[id_].means_.reshape([-1])\n",
    "                stds = np.sqrt(self.model[id_].covariances_).reshape([-1])\n",
    "                p_argmax = np.argmax(v, axis=1)\n",
    "                std_t = stds[p_argmax]\n",
    "                mean_t = means[p_argmax]\n",
    "                \n",
    "                # executing the inverse transformation \n",
    "                tmp = u * 4 * std_t + mean_t\n",
    "                \n",
    "                data_t[:, id_] = tmp\n",
    "                \n",
    "                # moving to the next set of columns in the raw generated data in correspondance to original column information\n",
    "                st += 1 + np.sum(self.components[id_])\n",
    "                \n",
    "            elif info['type'] == \"mixed\":\n",
    "                \n",
    "                # obtaining the generated normalized values and corresponding modes\n",
    "                u = data[:, st]\n",
    "                u = np.clip(u, -1, 1)\n",
    "                full_v = data[:,(st+1):(st+1)+len(info['modal'])+np.sum(self.components[id_])]\n",
    "                \n",
    "                # re-ordering the modes as per their original ordering\n",
    "                order = self.ordering[id_]\n",
    "                full_v_re_ordered = np.zeros_like(full_v)\n",
    "                for id,val in enumerate(order):\n",
    "                    full_v_re_ordered[:,val] = full_v[:,id]\n",
    "                full_v = full_v_re_ordered                \n",
    "                \n",
    "                # modes of categorical component\n",
    "                mixed_v = full_v[:,:len(info['modal'])]\n",
    "                \n",
    "                # modes of continuous component\n",
    "                v = full_v[:,-np.sum(self.components[id_]):]\n",
    "\n",
    "                # similarly ensuring un-used modes are represented with -100 to be ignored while computing argmax\n",
    "                v_t = np.ones((data.shape[0], self.n_clusters)) * -100\n",
    "                v_t[:, self.components[id_]] = v\n",
    "                v = np.concatenate([mixed_v,v_t], axis=1)       \n",
    "                p_argmax = np.argmax(v, axis=1)\n",
    "\n",
    "                # obtaining the means and stds of the continuous component using second fitted bgm model\n",
    "                means = self.model[id_][1].means_.reshape([-1]) \n",
    "                stds = np.sqrt(self.model[id_][1].covariances_).reshape([-1]) \n",
    "\n",
    "                # used to store the inverse-transformed data points\n",
    "                result = np.zeros_like(u)\n",
    "\n",
    "                for idx in range(len(data)):\n",
    "                    # in case of categorical mode being selected, the mode value itself is simply assigned \n",
    "                    if p_argmax[idx] < len(info['modal']):\n",
    "                        argmax_value = p_argmax[idx]\n",
    "                        result[idx] = float(list(map(info['modal'].__getitem__, [argmax_value]))[0])\n",
    "                    else:\n",
    "                        # in case of continuous mode being selected, similar inverse-transform for purely numeric values is applied\n",
    "                        std_t = stds[(p_argmax[idx]-len(info['modal']))]\n",
    "                        mean_t = means[(p_argmax[idx]-len(info['modal']))]\n",
    "                        result[idx] = u[idx] * 4 * std_t + mean_t\n",
    "            \n",
    "                data_t[:, id_] = result\n",
    "\n",
    "                st += 1 + np.sum(self.components[id_]) + len(info['modal'])\n",
    "                \n",
    "            else:\n",
    "                # reversing one hot encoding back to label encoding for categorical columns \n",
    "                current = data[:, st:st + info['size']]\n",
    "                idx = np.argmax(current, axis=1)\n",
    "                data_t[:, id_] = list(map(info['i2s'].__getitem__, idx))\n",
    "                st += info['size']\n",
    "        return data_t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b643c80-9e5a-48bd-be20-f266f4ed8a08",
   "metadata": {},
   "source": [
    "# Improving GMM encoding for continuous value column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb67b62-2b49-4cc5-a2eb-a1d3a086dbf3",
   "metadata": {},
   "source": [
    "Following we give an example of how to use our current gaussian mixture model to encode and decode continuous column. For this test, we have three demands:\n",
    "1. Current test is based on a small dataset. Please scale the code to enable GMM encoding on 1B rows data. This part is actually two sub-tasks (i) read 1B rows data into the algorithm and (ii) scale current GMM method to encode 1B rows data within a reasonable time.\n",
    "2. Properly evaluate the new GMM encoder. Make sure all the values can be inverse transformed. Especially the extreme values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524d927f-89ea-42e6-82b8-6c79bc81188c",
   "metadata": {},
   "source": [
    "## Load data and encode column using gaussian mixture method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fc166af-ea89-4844-80a3-fb6e56d718d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"Credit.csv\")[[\"Amount\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f22e0c44-b7bb-4f42-b832-aa45fc5b248a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>197.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Amount\n",
       "0   14.61\n",
       "1    1.00\n",
       "2  197.04\n",
       "3    1.00\n",
       "4   23.25"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e52c78f2-5e51-4548-9d1f-e1ae7660ef35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\mixture\\_base.py:270: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "transformer = DataTransformer(train_data=train_data)\n",
    "transformer.fit() \n",
    "transformed_train_data = transformer.transform(train_data.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725a2f37-2f1b-45f8-abdb-6d212a9d662b",
   "metadata": {},
   "source": [
    "### Show the encoding for value 14.61"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8791b6a0-b607-4e86-b8e1-b87817116d40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.44648112, 1.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        ])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4362ca-7963-4e69-9477-16bf1f06085e",
   "metadata": {},
   "source": [
    "## Inverse transform back the encoded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddee28f2-ff90-460e-b835-b8cb76bc0511",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_transformed_train_data = transformer.inverse_transform(transformed_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15757369-273e-4c0a-aecd-96089bf0571f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 14.61],\n",
       "       [  1.  ],\n",
       "       [197.04],\n",
       "       ...,\n",
       "       [ 12.  ],\n",
       "       [ 36.  ],\n",
       "       [108.  ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverse_transformed_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbee1f32-f571-4192-a197-4d83fd004fff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>197.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Amount\n",
       "0   14.61\n",
       "1    1.00\n",
       "2  197.04\n",
       "3    1.00\n",
       "4   23.25"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(inverse_transformed_train_data, columns=[\"Amount\"]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a8e6d4-562b-45b0-8b70-716f0cd0bc2d",
   "metadata": {},
   "source": [
    "# Scale code to handle 1B rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028bb569-198b-44fd-ad42-fb4549c52ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV using Dask\n",
    "chunk_size = 1_000_000\n",
    "dask_df = dd.read_csv(\"Credit.csv\", usecols=[\"Amount\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c3e3a05-a713-4034-bd03-4d8c874bdc68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled Dask DataFrame created with 1 billion rows.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dask\\dataframe\\core.py:8164: UserWarning: Insufficient elements for `head`. 1000000 elements requested, only 49842 elements available. Try passing larger `npartitions` to `head`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\mixture\\_base.py:270: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer initialized and fitted with sample metadata.\n",
      "Chunk Transforming Time: 0.00 seconds\n",
      "Chunk Transforming Time: 13.44 seconds\n",
      "Chunk Transforming Time: 14.24 seconds\n",
      "Chunk Transforming Time: 14.98 seconds\n",
      "Chunk Transforming Time: 15.05 seconds\n",
      "Chunk Transforming Time: 15.15 seconds\n",
      "Chunk Transforming Time: 15.18 seconds\n",
      "Chunk Transforming Time: 15.21 seconds\n",
      "Chunk Transforming Time: 15.27 seconds\n",
      "Chunk Transforming Time: 16.74 seconds\n",
      "Chunk Transforming Time: 16.94 seconds\n",
      "Chunk Transforming Time: 17.09 seconds\n",
      "Chunk Transforming Time: 17.11 seconds\n",
      "Chunk Transforming Time: 17.24 seconds\n",
      "Chunk Transforming Time: 17.24 seconds\n",
      "Chunk Transforming Time: 17.26 seconds\n",
      "Chunk Transforming Time: 17.24 seconds\n",
      "Chunk Transforming Time: 18.25 seconds\n",
      "Chunk Transforming Time: 18.20 seconds\n",
      "Chunk Transforming Time: 18.30 seconds\n",
      "Chunk Transforming Time: 18.26 seconds\n",
      "Chunk Transforming Time: 18.15 seconds\n",
      "Chunk Transforming Time: 18.18 seconds\n",
      "Chunk Transforming Time: 18.19 seconds\n",
      "Chunk Transforming Time: 18.17 seconds\n",
      "Chunk Transforming Time: 18.22 seconds\n",
      "Chunk Transforming Time: 18.31 seconds\n",
      "Chunk Transforming Time: 18.34 seconds\n",
      "Chunk Transforming Time: 18.34 seconds\n",
      "Chunk Transforming Time: 18.46 seconds\n",
      "Chunk Transforming Time: 18.36 seconds\n",
      "Chunk Transforming Time: 18.43 seconds\n",
      "Chunk Transforming Time: 18.44 seconds\n",
      "Chunk Transforming Time: 16.19 seconds\n",
      "Chunk Transforming Time: 16.11 seconds\n",
      "Chunk Transforming Time: 15.63 seconds\n",
      "Chunk Transforming Time: 16.06 seconds\n",
      "Chunk Transforming Time: 15.89 seconds\n",
      "Chunk Transforming Time: 15.85 seconds\n",
      "Chunk Transforming Time: 16.02 seconds\n",
      "Chunk Transforming Time: 16.10 seconds\n",
      "Chunk Transforming Time: 16.50 seconds\n",
      "Chunk Transforming Time: 16.92 seconds\n",
      "Chunk Transforming Time: 16.96 seconds\n",
      "Chunk Transforming Time: 16.94 seconds\n",
      "Chunk Transforming Time: 16.75 seconds\n",
      "Chunk Transforming Time: 16.92 seconds\n",
      "Chunk Transforming Time: 16.99 seconds\n",
      "Chunk Transforming Time: 16.94 seconds\n",
      "Chunk Transforming Time: 22.33 seconds\n",
      "Chunk Transforming Time: 22.62 seconds\n",
      "Chunk Transforming Time: 22.57 seconds\n",
      "Chunk Transforming Time: 22.22 seconds\n",
      "Chunk Transforming Time: 22.55 seconds\n",
      "Chunk Transforming Time: 22.63 seconds\n",
      "Chunk Transforming Time: 22.63 seconds\n",
      "Chunk Transforming Time: 22.60 seconds\n",
      "Chunk Transforming Time: 20.21 seconds\n",
      "Chunk Transforming Time: 19.86 seconds\n",
      "Chunk Transforming Time: 19.74 seconds\n",
      "Chunk Transforming Time: 19.93 seconds\n",
      "Chunk Transforming Time: 19.73 seconds\n",
      "Chunk Transforming Time: 19.98 seconds\n",
      "Chunk Transforming Time: 19.87 seconds\n",
      "Chunk Transforming Time: 19.94 seconds\n",
      "Chunk Transforming Time: 16.97 seconds\n",
      "Chunk Transforming Time: 17.20 seconds\n",
      "Chunk Transforming Time: 17.09 seconds\n",
      "Chunk Transforming Time: 16.99 seconds\n",
      "Chunk Transforming Time: 16.80 seconds\n",
      "Chunk Transforming Time: 17.04 seconds\n",
      "Chunk Transforming Time: 16.95 seconds\n",
      "Chunk Transforming Time: 17.03 seconds\n",
      "Chunk Transforming Time: 19.90 seconds\n",
      "Chunk Transforming Time: 19.76 seconds\n",
      "Chunk Transforming Time: 19.53 seconds\n",
      "Chunk Transforming Time: 19.89 seconds\n",
      "Chunk Transforming Time: 19.83 seconds\n",
      "Chunk Transforming Time: 19.63 seconds\n",
      "Chunk Transforming Time: 19.80 seconds\n",
      "Chunk Transforming Time: 19.68 seconds\n",
      "Chunk Transforming Time: 19.04 seconds\n",
      "Chunk Transforming Time: 19.05 seconds\n",
      "Chunk Transforming Time: 19.01 seconds\n",
      "Chunk Transforming Time: 18.82 seconds\n",
      "Chunk Transforming Time: 18.79 seconds\n",
      "Chunk Transforming Time: 18.83 seconds\n",
      "Chunk Transforming Time: 19.02 seconds\n",
      "Chunk Transforming Time: 19.04 seconds\n",
      "Chunk Transforming Time: 19.62 seconds\n",
      "Chunk Transforming Time: 19.68 seconds\n",
      "Chunk Transforming Time: 19.68 seconds\n",
      "Chunk Transforming Time: 19.64 seconds\n",
      "Chunk Transforming Time: 19.45 seconds\n",
      "Chunk Transforming Time: 19.69 seconds\n",
      "Chunk Transforming Time: 19.57 seconds\n",
      "Chunk Transforming Time: 19.66 seconds\n",
      "Chunk Transforming Time: 18.20 seconds\n",
      "Chunk Transforming Time: 17.69 seconds\n",
      "Chunk Transforming Time: 18.46 seconds\n",
      "Chunk Transforming Time: 18.35 seconds\n",
      "Chunk Transforming Time: 18.02 seconds\n",
      "Chunk Transforming Time: 18.16 seconds\n",
      "Chunk Transforming Time: 18.46 seconds\n",
      "Chunk Transforming Time: 18.38 seconds\n",
      "Chunk Transforming Time: 19.52 seconds\n",
      "Chunk Transforming Time: 19.39 seconds\n",
      "Chunk Transforming Time: 19.57 seconds\n",
      "Chunk Transforming Time: 19.65 seconds\n",
      "Chunk Transforming Time: 19.60 seconds\n",
      "Chunk Transforming Time: 19.60 seconds\n",
      "Chunk Transforming Time: 19.45 seconds\n",
      "Chunk Transforming Time: 19.79 seconds\n",
      "Chunk Transforming Time: 14.66 seconds\n",
      "Chunk Transforming Time: 14.76 seconds\n",
      "Chunk Transforming Time: 14.75 seconds\n",
      "Chunk Transforming Time: 14.62 seconds\n",
      "Chunk Transforming Time: 14.86 seconds\n",
      "Chunk Transforming Time: 14.81 seconds\n",
      "Chunk Transforming Time: 14.80 seconds\n",
      "Chunk Transforming Time: 14.71 seconds\n",
      "Chunk Transforming Time: 16.03 seconds\n",
      "Chunk Transforming Time: 15.99 seconds\n",
      "Chunk Transforming Time: 15.39 seconds\n",
      "Chunk Transforming Time: 15.12 seconds\n",
      "Chunk Transforming Time: 15.39 seconds\n",
      "Chunk Transforming Time: 15.42 seconds\n",
      "Chunk Transforming Time: 15.46 seconds\n",
      "Chunk Transforming Time: 15.55 seconds\n",
      "Chunk Transforming Time: 17.92 seconds\n",
      "Chunk Transforming Time: 17.92 seconds\n",
      "Chunk Transforming Time: 18.04 seconds\n",
      "Chunk Transforming Time: 18.04 seconds\n",
      "Chunk Transforming Time: 18.02 seconds\n",
      "Chunk Transforming Time: 17.89 seconds\n",
      "Chunk Transforming Time: 17.96 seconds\n",
      "Chunk Transforming Time: 17.92 seconds\n",
      "Chunk Transforming Time: 17.48 seconds\n",
      "Chunk Transforming Time: 17.20 seconds\n",
      "Chunk Transforming Time: 17.04 seconds\n",
      "Chunk Transforming Time: 17.24 seconds\n",
      "Chunk Transforming Time: 17.06 seconds\n",
      "Chunk Transforming Time: 17.37 seconds\n",
      "Chunk Transforming Time: 17.35 seconds\n",
      "Chunk Transforming Time: 17.29 seconds\n",
      "Chunk Transforming Time: 16.73 seconds\n",
      "Chunk Transforming Time: 16.74 seconds\n",
      "Chunk Transforming Time: 16.87 seconds\n",
      "Chunk Transforming Time: 17.05 seconds\n",
      "Chunk Transforming Time: 17.12 seconds\n",
      "Chunk Transforming Time: 17.15 seconds\n",
      "Chunk Transforming Time: 17.14 seconds\n",
      "Chunk Transforming Time: 17.30 seconds\n",
      "Chunk Transforming Time: 18.60 seconds\n",
      "Chunk Transforming Time: 18.48 seconds\n",
      "Chunk Transforming Time: 18.22 seconds\n",
      "Chunk Transforming Time: 18.02 seconds\n",
      "Chunk Transforming Time: 18.26 seconds\n",
      "Chunk Transforming Time: 18.23 seconds\n",
      "Chunk Transforming Time: 18.18 seconds\n",
      "Chunk Transforming Time: 18.16 seconds\n",
      "Chunk Transforming Time: 18.44 seconds\n",
      "Chunk Transforming Time: 18.88 seconds\n",
      "Chunk Transforming Time: 18.94 seconds\n",
      "Chunk Transforming Time: 18.72 seconds\n",
      "Chunk Transforming Time: 19.01 seconds\n",
      "Chunk Transforming Time: 19.24 seconds\n",
      "Chunk Transforming Time: 19.21 seconds\n",
      "Chunk Transforming Time: 19.22 seconds\n",
      "Chunk Transforming Time: 20.72 seconds\n",
      "Chunk Transforming Time: 20.78 seconds\n",
      "Chunk Transforming Time: 20.57 seconds\n",
      "Chunk Transforming Time: 20.66 seconds\n",
      "Chunk Transforming Time: 20.75 seconds\n",
      "Chunk Transforming Time: 20.59 seconds\n",
      "Chunk Transforming Time: 20.66 seconds\n",
      "Chunk Transforming Time: 20.56 seconds\n",
      "Chunk Transforming Time: 18.91 seconds\n",
      "Chunk Transforming Time: 18.76 seconds\n",
      "Chunk Transforming Time: 18.82 seconds\n",
      "Chunk Transforming Time: 18.79 seconds\n",
      "Chunk Transforming Time: 18.71 seconds\n",
      "Chunk Transforming Time: 18.90 seconds\n",
      "Chunk Transforming Time: 18.88 seconds\n",
      "Chunk Transforming Time: 18.88 seconds\n",
      "Chunk Transforming Time: 18.32 seconds\n",
      "Chunk Transforming Time: 18.08 seconds\n",
      "Chunk Transforming Time: 18.01 seconds\n",
      "Chunk Transforming Time: 18.08 seconds\n",
      "Chunk Transforming Time: 18.02 seconds\n",
      "Chunk Transforming Time: 18.23 seconds\n",
      "Chunk Transforming Time: 18.18 seconds\n",
      "Chunk Transforming Time: 18.48 seconds\n",
      "Chunk Transforming Time: 21.90 seconds\n",
      "Chunk Transforming Time: 21.97 seconds\n",
      "Chunk Transforming Time: 21.82 seconds\n",
      "Chunk Transforming Time: 21.86 seconds\n",
      "Chunk Transforming Time: 21.81 seconds\n",
      "Chunk Transforming Time: 22.08 seconds\n",
      "Chunk Transforming Time: 22.13 seconds\n",
      "Chunk Transforming Time: 22.25 seconds\n",
      "Chunk Transforming Time: 21.19 seconds\n",
      "Chunk Transforming Time: 21.72 seconds\n",
      "Chunk Transforming Time: 21.75 seconds\n",
      "Chunk Transforming Time: 21.99 seconds\n",
      "Chunk Transforming Time: 21.93 seconds\n",
      "Chunk Transforming Time: 22.64 seconds\n",
      "Chunk Transforming Time: 22.62 seconds\n",
      "Chunk Transforming Time: 22.52 seconds\n",
      "Chunk Transforming Time: 22.88 seconds\n",
      "Chunk Transforming Time: 22.26 seconds\n",
      "Chunk Transforming Time: 22.71 seconds\n",
      "Chunk Transforming Time: 22.59 seconds\n",
      "Chunk Transforming Time: 22.55 seconds\n",
      "Chunk Transforming Time: 21.56 seconds\n",
      "Chunk Transforming Time: 21.56 seconds\n",
      "Chunk Transforming Time: 21.20 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Process the data in Dask (transform in parallel without loading everything in memory)\u001b[39;00m\n\u001b[0;32m     28\u001b[0m start_total_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 29\u001b[0m transformed_chunks \u001b[38;5;241m=\u001b[39m \u001b[43mscaled_dask_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_partitions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m total_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_total_time\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal Processing Time for All Chunks: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dask\\base.py:372\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    349\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[0;32m    350\u001b[0m \n\u001b[0;32m    351\u001b[0m \u001b[38;5;124;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;124;03m    dask.compute\u001b[39;00m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 372\u001b[0m     (result,) \u001b[38;5;241m=\u001b[39m \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    373\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dask\\base.py:660\u001b[0m, in \u001b[0;36mcompute\u001b[1;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[0;32m    657\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[0;32m    659\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m shorten_traceback():\n\u001b[1;32m--> 660\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    662\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[0;32m    179\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m--> 180\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    181\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 324\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    326\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk Transforming Time: 18.40 seconds\n",
      "Chunk Transforming Time: 18.61 seconds\n",
      "Chunk Transforming Time: 18.65 seconds\n",
      "Chunk Transforming Time: 18.77 seconds\n",
      "Chunk Transforming Time: 18.57 seconds\n",
      "Chunk Transforming Time: 17.48 seconds\n",
      "Chunk Transforming Time: 17.24 seconds\n",
      "Chunk Transforming Time: 17.45 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dask\\dataframe\\core.py:8164: UserWarning: Insufficient elements for `head`. 1000000 elements requested, only 49842 elements available. Try passing larger `npartitions` to `head`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dask\\dataframe\\core.py:8164: UserWarning: Insufficient elements for `head`. 1000000 elements requested, only 49842 elements available. Try passing larger `npartitions` to `head`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Scale up the data to simulate 1 billion rows\n",
    "scaled_dask_df = dd.concat([dask_df] * (1_000_000_000 // len(dask_df)), interleave_partitions=True)\n",
    "\n",
    "# Convert the Dask DataFrame to a lower memory type if necessary\n",
    "scaled_dask_df[\"Amount\"] = scaled_dask_df[\"Amount\"].astype(np.float32)\n",
    "\n",
    "print(\"Scaled Dask DataFrame created with 1 billion rows.\")\n",
    "\n",
    "# Initialize the transformer (assuming you have a class called DataTransformer)\n",
    "sample_df = scaled_dask_df.head(chunk_size, compute=True)  # Load a sample to initialize\n",
    "transformer = DataTransformer(train_data=sample_df)\n",
    "transformer.fit()\n",
    "print(\"Transformer initialized and fitted with sample metadata.\")\n",
    "\n",
    "# Define the function to transform data chunks with timing\n",
    "def transform_chunk(chunk, transformer):\n",
    "    start_time = time.time()\n",
    "    transformed_chunk = transformer.transform(chunk.values)\n",
    "    transform_time = time.time() - start_time\n",
    "    print(f\"Chunk Transforming Time: {transform_time:.2f} seconds\")\n",
    "    return transformed_chunk\n",
    "\n",
    "# Process the data in Dask (transform in parallel without loading everything in memory)\n",
    "start_total_time = time.time()\n",
    "transformed_chunks = scaled_dask_df.map_partitions(lambda df: transform_chunk(df, transformer)).compute()\n",
    "total_time = time.time() - start_total_time\n",
    "print(f\"Total Processing Time for All Chunks: {total_time:.2f} seconds\")\n",
    "\n",
    "# Further processing and analysis would follow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288554c6-a513-4f07-b1a5-2d8a62246ca5",
   "metadata": {},
   "source": [
    "# Validation on Extreme Values and inverse transformation evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de3888b8-fd37-43a9-a591-b244d2764a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "extreme_values = pd.DataFrame([[0], [1e10]], columns=[\"Amount\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54bf7e47-d6dd-434d-a23f-9a95f6d32a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extreme Values Transforming Time: 0.001002 seconds\n",
      "Transformed Extreme Values:\n",
      "[[-0.26713135  1.          0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [ 0.99        0.          1.          0.          0.          0.\n",
      "   0.          0.        ]]\n",
      "Extreme Values Inverse Transforming Time: 0.000000 seconds\n",
      "Inverse Transformed Extreme Values:\n",
      "[[ 0.        ]\n",
      " [82.88158181]]\n",
      "MAE on Extreme Values: 4999999958.559209\n",
      "RMSE on Extreme Values: 7071067753.259346\n",
      "Starting full dataset transformation...\n",
      "Processed chunk 1, shape: (49842, 1)\n",
      "Sample transformed data from chunk 1:\n",
      "[[-0.22937002  0.          1.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [-0.21828724  1.          0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [ 0.06976218  0.          0.          0.          1.          0.\n",
      "   0.          0.        ]\n",
      " [-0.21828724  1.          0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [-0.07505461  0.          1.          0.          0.          0.\n",
      "   0.          0.        ]]\n",
      "Processed chunk 2, shape: (49842, 1)\n",
      "Sample transformed data from chunk 2:\n",
      "[[-0.22937002  0.          1.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [-0.21828724  1.          0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [ 0.06976218  0.          0.          0.          1.          0.\n",
      "   0.          0.        ]\n",
      " [-0.21828724  1.          0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [-0.07505461  0.          1.          0.          0.          0.\n",
      "   0.          0.        ]]\n",
      "Processed chunk 3, shape: (49842, 1)\n",
      "Sample transformed data from chunk 3:\n",
      "[[-0.22937002  0.          1.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [-0.21828724  1.          0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [ 0.06976218  0.          0.          0.          1.          0.\n",
      "   0.          0.        ]\n",
      " [-0.4724525   0.          1.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [-0.07505461  0.          1.          0.          0.          0.\n",
      "   0.          0.        ]]\n",
      "Processed chunk 4, shape: (49842, 1)\n",
      "Sample transformed data from chunk 4:\n",
      "[[ 0.4464811   1.          0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [-0.21828724  1.          0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [ 0.06976218  0.          0.          0.          1.          0.\n",
      "   0.          0.        ]\n",
      " [-0.21828724  1.          0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [-0.07505461  0.          1.          0.          0.          0.\n",
      "   0.          0.        ]]\n",
      "Processed chunk 5, shape: (49842, 1)\n",
      "Sample transformed data from chunk 5:\n",
      "[[-0.22937002  0.          1.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [-0.4724525   0.          1.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [ 0.06976218  0.          0.          0.          1.          0.\n",
      "   0.          0.        ]\n",
      " [-0.21828724  1.          0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [-0.07505461  0.          1.          0.          0.          0.\n",
      "   0.          0.        ]]\n",
      "Processed chunk 6, shape: (49842, 1)\n",
      "Sample transformed data from chunk 6:\n",
      "[[-0.22937002  0.          1.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [-0.4724525   0.          1.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [ 0.06976218  0.          0.          0.          1.          0.\n",
      "   0.          0.        ]\n",
      " [-0.21828724  1.          0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [-0.07505461  0.          1.          0.          0.          0.\n",
      "   0.          0.        ]]\n",
      "Processed chunk 7, shape: (49842, 1)\n",
      "Sample transformed data from chunk 7:\n",
      "[[ 0.4464811   1.          0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [-0.21828724  1.          0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [ 0.06976218  0.          0.          0.          1.          0.\n",
      "   0.          0.        ]\n",
      " [-0.21828724  1.          0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [-0.07505461  0.          1.          0.          0.          0.\n",
      "   0.          0.        ]]\n",
      "Processed chunk 8, shape: (49842, 1)\n",
      "Sample transformed data from chunk 8:\n",
      "[[-0.22937002  0.          1.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [-0.21828724  1.          0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [ 0.06976218  0.          0.          0.          1.          0.\n",
      "   0.          0.        ]\n",
      " [-0.21828724  1.          0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [-0.41487107  0.          0.          1.          0.          0.\n",
      "   0.          0.        ]]\n",
      "Processed chunk 9, shape: (49842, 1)\n",
      "Sample transformed data from chunk 9:\n",
      "[[-0.48232937  0.          0.          1.          0.          0.\n",
      "   0.          0.        ]\n",
      " [-0.21828724  1.          0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [ 0.06976218  0.          0.          0.          1.          0.\n",
      "   0.          0.        ]\n",
      " [-0.21828724  1.          0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [-0.41487107  0.          0.          1.          0.          0.\n",
      "   0.          0.        ]]\n",
      "Processed chunk 10, shape: (49842, 1)\n",
      "Sample transformed data from chunk 10:\n",
      "[[ 0.4464811   1.          0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [-0.21828724  1.          0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [ 0.06976218  0.          0.          0.          1.          0.\n",
      "   0.          0.        ]\n",
      " [-0.21828724  1.          0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [-0.07505461  0.          1.          0.          0.          0.\n",
      "   0.          0.        ]]\n",
      "Processed chunk 11, shape: (49842, 1)\n",
      "Sample transformed data from chunk 11:\n",
      "[[-0.22937002  0.          1.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [-0.21828724  1.          0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [ 0.06976218  0.          0.          0.          1.          0.\n",
      "   0.          0.        ]\n",
      " [-0.21828724  1.          0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [-0.07505461  0.          1.          0.          0.          0.\n",
      "   0.          0.        ]]\n",
      "Processed chunk 12, shape: (49842, 1)\n",
      "Sample transformed data from chunk 12:\n",
      "[[-0.22937002  0.          1.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [-0.21828724  1.          0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [ 0.06976218  0.          0.          0.          1.          0.\n",
      "   0.          0.        ]\n",
      " [-0.21828724  1.          0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [-0.07505461  0.          1.          0.          0.          0.\n",
      "   0.          0.        ]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 34\u001b[0m\n\u001b[0;32m     32\u001b[0m transformed_chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m partition \u001b[38;5;129;01min\u001b[39;00m scaled_dask_df\u001b[38;5;241m.\u001b[39mto_delayed():\n\u001b[1;32m---> 34\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m \u001b[43mpartition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m     transformed_chunk \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mtransform(chunk\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[0;32m     36\u001b[0m     transformed_chunks\u001b[38;5;241m.\u001b[39mappend(transformed_chunk)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dask\\base.py:372\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    349\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[0;32m    350\u001b[0m \n\u001b[0;32m    351\u001b[0m \u001b[38;5;124;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;124;03m    dask.compute\u001b[39;00m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 372\u001b[0m     (result,) \u001b[38;5;241m=\u001b[39m \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    373\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dask\\base.py:660\u001b[0m, in \u001b[0;36mcompute\u001b[1;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[0;32m    657\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[0;32m    659\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m shorten_traceback():\n\u001b[1;32m--> 660\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    662\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[0;32m    179\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m--> 180\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    181\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 324\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    326\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Measure the time for transforming extreme values\n",
    "start_time = time.time()\n",
    "transformed_extreme_values = transformer.transform(extreme_values.values)\n",
    "transform_time = time.time() - start_time\n",
    "print(f\"Extreme Values Transforming Time: {transform_time:.6f} seconds\")\n",
    "print(\"Transformed Extreme Values:\")\n",
    "print(transformed_extreme_values)\n",
    "\n",
    "# Measure the time for inverse transforming extreme values\n",
    "start_time = time.time()\n",
    "inverse_extreme_values = transformer.inverse_transform(transformed_extreme_values)\n",
    "inverse_transform_time = time.time() - start_time\n",
    "print(f\"Extreme Values Inverse Transforming Time: {inverse_transform_time:.6f} seconds\")\n",
    "print(\"Inverse Transformed Extreme Values:\")\n",
    "print(inverse_extreme_values)\n",
    "\n",
    "# Calculate and display error metrics for extreme values\n",
    "mae_extreme = mean_absolute_error(extreme_values, inverse_extreme_values)\n",
    "rmse_extreme = np.sqrt(mean_squared_error(extreme_values, inverse_extreme_values))\n",
    "print(f\"MAE on Extreme Values: {mae_extreme}\")\n",
    "print(f\"RMSE on Extreme Values: {rmse_extreme}\")\n",
    "\n",
    "# Transform and evaluate the full dataset in smaller, manageable parts with intermediate prints\n",
    "print(\"Starting full dataset transformation...\")\n",
    "start_time = time.time()\n",
    "chunk_count = 0\n",
    "\n",
    "# Transform the data chunk by chunk with intermediate printouts\n",
    "transformed_chunks = []\n",
    "for partition in scaled_dask_df.to_delayed():\n",
    "    chunk = partition.compute()\n",
    "    transformed_chunk = transformer.transform(chunk.values)\n",
    "    transformed_chunks.append(transformed_chunk)\n",
    "    chunk_count += 1\n",
    "    print(f\"Processed chunk {chunk_count}, shape: {chunk.shape}\")\n",
    "    print(f\"Sample transformed data from chunk {chunk_count}:\")\n",
    "    print(transformed_chunk[:5])  # Print first 5 rows of the transformed chunk\n",
    "\n",
    "full_transform_time = time.time() - start_time\n",
    "print(f\"Full Dataset Transforming Time: {full_transform_time:.2f} seconds\")\n",
    "\n",
    "# Measure the time for inverse transforming the full dataset\n",
    "print(\"Starting full dataset inverse transformation...\")\n",
    "start_time = time.time()\n",
    "inverse_transformed_chunks = []\n",
    "for transformed_chunk in transformed_chunks:\n",
    "    inverse_chunk = transformer.inverse_transform(transformed_chunk)\n",
    "    inverse_transformed_chunks.append(inverse_chunk)\n",
    "    print(\"Inverse transformed a chunk.\")\n",
    "    print(\"Sample inverse transformed data:\")\n",
    "    print(inverse_chunk[:5])  # Print first 5 rows of the inverse transformed chunk\n",
    "\n",
    "full_inverse_transform_time = time.time() - start_time\n",
    "print(f\"Full Dataset Inverse Transforming Time: {full_inverse_transform_time:.2f} seconds\")\n",
    "\n",
    "# Flatten and compute metrics for the full dataset\n",
    "flat_original = np.concatenate([chunk.compute().values.flatten() for chunk in scaled_dask_df.to_delayed()])\n",
    "flat_inverse = np.concatenate([chunk.flatten() for chunk in inverse_transformed_chunks])\n",
    "\n",
    "# Calculate and display error metrics for the full dataset\n",
    "mae_full = mean_absolute_error(flat_original, flat_inverse)\n",
    "rmse_full = np.sqrt(mean_squared_error(flat_original, flat_inverse))\n",
    "print(f\"Full Dataset MAE: {mae_full}\")\n",
    "print(f\"Full Dataset RMSE: {rmse_full}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d9c77b-92d5-4076-bab5-5212b5726686",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
